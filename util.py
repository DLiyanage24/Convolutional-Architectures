# -*- coding: utf-8 -*-
"""util.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C3k7l9uafSZQl8AFoN2NjVxb3ukZHAVY
"""

# util.py - Helper functions for CIFAR-100 CNN Analysis
# Contains data loading, preprocessing, evaluation, and visualization functions

from __future__ import print_function
import random
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
from scipy import stats
import pickle
import os

# Fix seeds for reproducibility
SEED = 2025
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# CIFAR-100 class names
cifar100_classes = [
    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',
    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',
    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',
    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',
    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',
    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',
    'worm'
]

# CIFAR-100 Hierarchical Information
cifar100_superclass_names = [
    'aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_and_vegetables',
    'household_electrical_devices', 'household_furniture', 'insects', 'large_carnivores',
    'large_man-made_outdoor_things', 'large_natural_outdoor_scenes', 'large_omnivores_and_herbivores',
    'medium_mammals', 'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals',
    'trees', 'vehicles_1', 'vehicles_2'
]

cifar100_fine_to_superclass = [
    4, 1, 14, 8, 0, 6, 7, 7, 18, 3, 3, 14, 9, 18, 7, 11, 3, 9, 7, 11,
    6, 11, 5, 10, 7, 6, 13, 15, 3, 15, 0, 11, 1, 10, 12, 14, 16, 9, 11, 5,
    5, 19, 8, 8, 15, 13, 14, 17, 18, 10, 16, 4, 17, 4, 2, 0, 17, 4, 18, 17,
    10, 3, 2, 12, 12, 16, 12, 1, 9, 19, 2, 10, 0, 1, 16, 12, 9, 13, 15, 13,
    16, 19, 2, 4, 6, 19, 5, 5, 8, 19, 18, 1, 2, 15, 6, 0, 17, 8, 14, 13
]

def load_cifar100_data(data_dir='./tensorflow-datasets/', batch_size=64):
    """Load and prepare CIFAR-100 dataset."""
    print("Loading CIFAR-100 dataset...")

    train = tfds.load('cifar100', split='train[:90%]', data_dir=data_dir)
    validation = tfds.load('cifar100', split='train[-10%:]', data_dir=data_dir)
    test = tfds.load('cifar100', split='test', data_dir=data_dir)

    # Create train, validation and test sets with minibatching
    train_ds = train.shuffle(1024).batch(batch_size)
    val_ds = validation.batch(batch_size)
    test_ds = test.batch(batch_size)

    return train_ds, val_ds, test_ds

def preprocess_data(batch):
    """Preprocess CIFAR-100 data: normalize images and convert labels to one-hot."""
    images = tf.cast(batch['image'], tf.float32) / 255.0
    labels = tf.one_hot(batch['label'], depth=100)
    return images, labels

def preprocess_hierarchical_data(batch):
    """Preprocess data with both superclass and fine class labels."""
    images = tf.cast(batch['image'], tf.float32) / 255.0
    fine_labels = tf.one_hot(batch['label'], depth=100)

    # Create superclass labels from fine labels
    fine_labels_int = batch['label']
    superclass_labels_int = tf.gather(cifar100_fine_to_superclass, fine_labels_int)
    superclass_labels = tf.one_hot(superclass_labels_int, depth=20)

    return images, {'superclass_output': superclass_labels, 'fine_class_output': fine_labels}

def evaluate_test_accuracy(model, test_ds):
    """Evaluate model performance on test set including top-5 accuracy."""
    test_data = test_ds.map(preprocess_data)

    test_predictions = []
    test_labels = []
    test_probabilities = []

    for batch in test_data:
        images_test, labels_test = batch
        labels_int = tf.argmax(labels_test, axis=1)

        predictions_test = model(images_test, training=False)
        predicted = tf.argmax(predictions_test, axis=1)

        test_predictions.extend(predicted.numpy())
        test_labels.extend(labels_int.numpy())
        test_probabilities.extend(predictions_test.numpy())

    # Calculate top-1 accuracy
    test_acc = np.mean(np.array(test_predictions) == np.array(test_labels))

    # Calculate top-5 accuracy
    test_probabilities_array = np.array(test_probabilities)
    test_labels_array = np.array(test_labels)

    # Get top 5 predictions for each sample
    top5_predictions = np.argsort(test_probabilities_array, axis=1)[:, -5:]

    # Check if true label is in top 5 for each sample
    top5_correct = np.array([true_label in top5_pred for true_label, top5_pred in
                            zip(test_labels_array, top5_predictions)])

    top5_acc = np.mean(top5_correct)

    return test_acc, top5_acc, test_predictions, test_labels, test_probabilities

def evaluate_hierarchical_model(model, test_ds):
    """Comprehensive evaluation of hierarchical model."""
    test_data = test_ds.map(preprocess_hierarchical_data)

    superclass_preds = []
    fine_class_preds = []
    superclass_true = []
    fine_class_true = []
    superclass_probs = []
    fine_class_probs = []

    print("Evaluating on test set...")
    for batch in test_data:
        images, labels = batch
        superclass_true_batch = tf.argmax(labels['superclass_output'], axis=1)
        fine_class_true_batch = tf.argmax(labels['fine_class_output'], axis=1)

        # Get predictions
        predictions = model(images, training=False)
        superclass_pred_probs, fine_class_pred_probs = predictions

        superclass_pred_batch = tf.argmax(superclass_pred_probs, axis=1)
        fine_class_pred_batch = tf.argmax(fine_class_pred_probs, axis=1)

        # Store results
        superclass_preds.extend(superclass_pred_batch.numpy())
        fine_class_preds.extend(fine_class_pred_batch.numpy())
        superclass_true.extend(superclass_true_batch.numpy())
        fine_class_true.extend(fine_class_true_batch.numpy())
        superclass_probs.extend(superclass_pred_probs.numpy())
        fine_class_probs.extend(fine_class_pred_probs.numpy())

    # Calculate accuracies
    superclass_acc = np.mean(np.array(superclass_preds) == np.array(superclass_true))
    fine_class_acc = np.mean(np.array(fine_class_preds) == np.array(fine_class_true))

    # Calculate top-5 accuracy for fine classes
    fine_class_probs_array = np.array(fine_class_probs)
    fine_class_true_array = np.array(fine_class_true)
    top5_predictions = np.argsort(fine_class_probs_array, axis=1)[:, -5:]
    top5_correct = np.array([true_label in top5_pred for true_label, top5_pred in
                            zip(fine_class_true_array, top5_predictions)])
    fine_class_top5_acc = np.mean(top5_correct)

    # Hierarchical accuracy (correct superclass AND fine class)
    hierarchical_correct = np.logical_and(
        np.array(superclass_preds) == np.array(superclass_true),
        np.array(fine_class_preds) == np.array(fine_class_true)
    )
    hierarchical_acc = np.mean(hierarchical_correct)

    return {
        'superclass_acc': superclass_acc,
        'fine_class_acc': fine_class_acc,
        'fine_class_top5_acc': fine_class_top5_acc,
        'hierarchical_acc': hierarchical_acc,
        'superclass_preds': superclass_preds,
        'fine_class_preds': fine_class_preds,
        'superclass_true': superclass_true,
        'fine_class_true': fine_class_true,
        'superclass_probs': superclass_probs,
        'fine_class_probs': fine_class_probs
    }

def calculate_confidence_interval(accuracy, n_samples, confidence_level=0.95):
    """Calculate confidence interval for generalization error."""
    error_rate = 1 - accuracy
    se = np.sqrt(error_rate * (1 - error_rate) / n_samples)
    alpha = 1 - confidence_level
    z_score = stats.norm.ppf(1 - alpha/2)
    margin_of_error = z_score * se
    lower_error = max(0, error_rate - margin_of_error)
    upper_error = min(1, error_rate + margin_of_error)
    return lower_error, upper_error

def plot_training_curves(all_results):
    """Plot training curves for all experiments."""
    n_experiments = len(all_results)
    n_cols = 4
    n_rows = (n_experiments + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    axes = axes.flatten()

    for i, (exp_name, result) in enumerate(all_results.items()):
        ax = axes[i]

        epochs = range(1, len(result['train_accs']) + 1)

        ax.plot(epochs, result['train_accs'], 'o-', color='blue',
                linewidth=2, markersize=3, label='Train Acc', alpha=0.8)
        ax.plot(epochs, result['val_accs'], 's--', color='red',
                linewidth=2, markersize=3, label='Val Acc', alpha=0.8)
        ax.axhline(y=result['test_acc'], color='green', linestyle=':',
                   linewidth=2, label=f"Test: {result['test_acc']:.3f}", alpha=0.8)

        ax.set_title(f"{exp_name}\n{result['optimizer'].upper()}, {result['regularizer']}",
                    fontsize=9, fontweight='bold')
        ax.set_xlabel('Epochs')
        ax.set_ylabel('Accuracy')
        ax.legend(loc='lower right', fontsize=8)
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1.0)

    # Hide unused subplots
    for i in range(len(all_results), len(axes)):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.show()

def plot_comparison_summary(all_results):
    """Create summary comparison plots."""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    exp_names = list(all_results.keys())
    test_accs = [all_results[exp]['test_acc'] for exp in exp_names]
    top5_accs = [all_results[exp]['top5_acc'] for exp in exp_names]

    # 1. Test Accuracy Comparison (Top-1 vs Top-5)
    x = np.arange(len(exp_names))
    width = 0.35

    bars1 = ax1.bar(x - width/2, test_accs, width, label='Top-1 Accuracy', alpha=0.8, color='skyblue')
    bars2 = ax1.bar(x + width/2, top5_accs, width, label='Top-5 Accuracy', alpha=0.8, color='lightcoral')

    ax1.set_xlabel('Experiments')
    ax1.set_ylabel('Accuracy')
    ax1.set_title('Top-1 vs Top-5 Accuracy Comparison', fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(exp_names, rotation=45, ha='right', fontsize=8)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. Architecture Comparison
    arch1_accs = [v['test_acc'] for k, v in all_results.items() if v['architecture'] == 1]
    arch2_accs = [v['test_acc'] for k, v in all_results.items() if v['architecture'] == 2]
    arch1_top5 = [v['top5_acc'] for k, v in all_results.items() if v['architecture'] == 1]
    arch2_top5 = [v['top5_acc'] for k, v in all_results.items() if v['architecture'] == 2]

    bp1 = ax2.boxplot([arch1_accs, arch2_accs], positions=[1, 2], widths=0.3,
                      patch_artist=True, labels=['Arch 1 (Top-1)', 'Arch 2 (Top-1)'])
    bp2 = ax2.boxplot([arch1_top5, arch2_top5], positions=[1.4, 2.4], widths=0.3,
                      patch_artist=True, labels=['Arch 1 (Top-5)', 'Arch 2 (Top-5)'])

    for patch in bp1['boxes']:
        patch.set_facecolor('skyblue')
        patch.set_alpha(0.7)
    for patch in bp2['boxes']:
        patch.set_facecolor('lightcoral')
        patch.set_alpha(0.7)

    ax2.set_ylabel('Accuracy')
    ax2.set_title('Architecture Comparison (Top-1 vs Top-5)', fontweight='bold')
    ax2.set_xticks([1.2, 2.2])
    ax2.set_xticklabels(['Architecture 1', 'Architecture 2'])
    ax2.grid(True, alpha=0.3)

    # 3. Optimizer Comparison
    adam_accs = [v['test_acc'] for k, v in all_results.items() if v['optimizer'] == 'adam']
    sgd_accs = [v['test_acc'] for k, v in all_results.items() if v['optimizer'] == 'sgd']
    adam_top5 = [v['top5_acc'] for k, v in all_results.items() if v['optimizer'] == 'adam']
    sgd_top5 = [v['top5_acc'] for k, v in all_results.items() if v['optimizer'] == 'sgd']

    bp3 = ax3.boxplot([adam_accs, sgd_accs], positions=[1, 2], widths=0.3,
                      patch_artist=True, labels=['Adam (Top-1)', 'SGD (Top-1)'])
    bp4 = ax3.boxplot([adam_top5, sgd_top5], positions=[1.4, 2.4], widths=0.3,
                      patch_artist=True, labels=['Adam (Top-5)', 'SGD (Top-5)'])

    for patch in bp3['boxes']:
        patch.set_facecolor('skyblue')
        patch.set_alpha(0.7)
    for patch in bp4['boxes']:
        patch.set_facecolor('lightcoral')
        patch.set_alpha(0.7)

    ax3.set_ylabel('Accuracy')
    ax3.set_title('Optimizer Comparison (Top-1 vs Top-5)', fontweight='bold')
    ax3.set_xticks([1.2, 2.2])
    ax3.set_xticklabels(['Adam', 'SGD'])
    ax3.grid(True, alpha=0.3)

    # 4. Regularization Comparison
    no_reg_accs = [v['test_acc'] for k, v in all_results.items() if v['regularizer'] == 'No_Reg']
    l2_reg_accs = [v['test_acc'] for k, v in all_results.items() if v['regularizer'] == 'L2_Reg']
    no_reg_top5 = [v['top5_acc'] for k, v in all_results.items() if v['regularizer'] == 'No_Reg']
    l2_reg_top5 = [v['top5_acc'] for k, v in all_results.items() if v['regularizer'] == 'L2_Reg']

    bp5 = ax4.boxplot([no_reg_accs, l2_reg_accs], positions=[1, 2], widths=0.3,
                      patch_artist=True, labels=['No Reg (Top-1)', 'L2 Reg (Top-1)'])
    bp6 = ax4.boxplot([no_reg_top5, l2_reg_top5], positions=[1.4, 2.4], widths=0.3,
                      patch_artist=True, labels=['No Reg (Top-5)', 'L2 Reg (Top-5)'])

    for patch in bp5['boxes']:
        patch.set_facecolor('skyblue')
        patch.set_alpha(0.7)
    for patch in bp6['boxes']:
        patch.set_facecolor('lightcoral')
        patch.set_alpha(0.7)

    ax4.set_ylabel('Accuracy')
    ax4.set_title('Regularization Comparison (Top-1 vs Top-5)', fontweight='bold')
    ax4.set_xticks([1.2, 2.2])
    ax4.set_xticklabels(['No Regularization', 'L2 Regularization'])
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def plot_generalization_error_bars(all_results):
    """Create bar graph showing generalization error with confidence intervals."""
    # Extract data for all experiments
    experiment_data = []

    for exp_name, result in all_results.items():
        # Calculate generalization error (1 - test_accuracy)
        generalization_error = 1 - result['test_acc']

        # Get confidence interval bounds (already calculated as error rates)
        ci_lower, ci_upper = result['confidence_interval']

        # Store experiment info
        experiment_data.append({
            'name': exp_name,
            'gen_error': generalization_error,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'test_acc': result['test_acc'],
            'architecture': result['architecture'],
            'optimizer': result['optimizer'],
            'regularizer': result['regularizer']
        })

    # Sort by generalization error (ascending - best models first)
    experiment_data.sort(key=lambda x: x['gen_error'])

    # Extract data for plotting
    model_names = [exp['name'] for exp in experiment_data]
    gen_errors = [exp['gen_error'] for exp in experiment_data]
    ci_lowers = [exp['ci_lower'] for exp in experiment_data]
    ci_uppers = [exp['ci_upper'] for exp in experiment_data]

    # Calculate error bars (distance from point to CI bounds)
    error_lower = [gen_err - ci_low for gen_err, ci_low in zip(gen_errors, ci_lowers)]
    error_upper = [ci_up - gen_err for gen_err, ci_up in zip(gen_errors, ci_uppers)]

    # Create figure
    fig, ax = plt.subplots(figsize=(16, 8))

    # Create colors based on model characteristics
    colors = []
    for exp in experiment_data:
        if exp['architecture'] == 1:
            if exp['optimizer'] == 'adam':
                colors.append('lightblue')
            else:
                colors.append('lightcyan')
        else:  # architecture == 2
            if exp['optimizer'] == 'adam':
                colors.append('lightcoral')
            else:
                colors.append('lightpink')

    # Create horizontal bar plot
    y_pos = range(len(model_names))
    bars = ax.barh(y_pos, gen_errors, xerr=[error_lower, error_upper],
                   capsize=5, color=colors, alpha=0.8,
                   edgecolor='black', linewidth=0.5)

    # Customize plot
    ax.set_ylabel('Model Configurations', fontsize=12, fontweight='bold')
    ax.set_xlabel('Generalization Error', fontsize=12, fontweight='bold')
    ax.set_title('Generalization Error with 95% Confidence Intervals\n(All 16 CNN Model Configurations)',
                 fontsize=14, fontweight='bold')

    # Set y-axis labels
    ax.set_yticks(y_pos)
    ax.set_yticklabels([name.replace('_', ' ') for name in model_names], fontsize=9)

    # Add grid
    ax.grid(True, alpha=0.3, axis='x')
    ax.set_axisbelow(True)

    # Add legend for colors
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='lightblue', alpha=0.8, label='Arch 1 + Adam'),
        Patch(facecolor='lightcyan', alpha=0.8, label='Arch 1 + SGD'),
        Patch(facecolor='lightcoral', alpha=0.8, label='Arch 2 + Adam'),
        Patch(facecolor='lightpink', alpha=0.8, label='Arch 2 + SGD')
    ]
    ax.legend(handles=legend_elements, loc='lower right', fontsize=10)

    # Add value labels on bars
    for i, (bar, gen_err, test_acc) in enumerate(zip(bars, gen_errors, [exp['test_acc'] for exp in experiment_data])):
        width = bar.get_width()
        ax.text(width + 0.005, bar.get_y() + bar.get_height()/2.,
                f'{gen_err:.3f}', ha='left', va='center', fontsize=8, fontweight='bold')
        ax.text(width - 0.005, bar.get_y() + bar.get_height()/2.,
                f'({test_acc:.3f})', ha='right', va='center', fontsize=7,
                style='italic', color='darkblue')

    # Set x-axis limits with some padding
    max_error = max([ci_up for ci_up in ci_uppers])
    ax.set_xlim(0, max_error + 0.03)

    # Add vertical line at mean generalization error
    mean_gen_error = np.mean(gen_errors)
    ax.axvline(x=mean_gen_error, color='red', linestyle='--', alpha=0.7, linewidth=2)

    plt.tight_layout()
    plt.show()

    return experiment_data

def plot_confusion_matrices(all_results):
    """Plot confusion matrices for top 5 models."""
    sorted_results = sorted(all_results.items(), key=lambda x: x[1]['test_acc'], reverse=True)
    top5_results = dict(sorted_results[:5])

    fig, axes = plt.subplots(1, 5, figsize=(25, 5))

    for i, (exp_name, result) in enumerate(top5_results.items()):
        cm = result['confusion_matrix']
        cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)

        ax = axes[i]
        sns.heatmap(cm_normalized,
                    annot=False,
                    cmap='Blues',
                    square=True,
                    ax=ax,
                    cbar=i==4)  # Only show colorbar on last plot

        ax.set_title(f"{exp_name}\nAcc: {result['test_acc']:.4f}",
                     fontweight='bold', fontsize=12)
        ax.set_xlabel('Predicted', fontsize=10)
        ax.set_ylabel('True' if i == 0 else '', fontsize=10)
        ax.set_xticks([])
        ax.set_yticks([])

    plt.suptitle('Confusion Matrix Comparison - Top 5 Models',
                 fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

def plot_hierarchical_results(history, eval_results, n_samples, best_single_acc):
    """Plot comprehensive results for hierarchical model."""

    fig = plt.figure(figsize=(20, 15))

    # 1. Training curves
    ax1 = plt.subplot(3, 3, 1)
    epochs = range(1, len(history.history['loss']) + 1)
    plt.plot(epochs, history.history['superclass_output_accuracy'], 'b-', label='Train Superclass', linewidth=2)
    plt.plot(epochs, history.history['val_superclass_output_accuracy'], 'b--', label='Val Superclass', linewidth=2)
    plt.plot(epochs, history.history['fine_class_output_accuracy'], 'r-', label='Train Fine Class', linewidth=2)
    plt.plot(epochs, history.history['val_fine_class_output_accuracy'], 'r--', label='Val Fine Class', linewidth=2)
    plt.axhline(y=eval_results['superclass_acc'], color='blue', linestyle=':', alpha=0.8, label='Test Superclass')
    plt.axhline(y=eval_results['fine_class_acc'], color='red', linestyle=':', alpha=0.8, label='Test Fine Class')
    plt.title('Training Curves: Hierarchical Model', fontweight='bold', fontsize=14)
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 2. Loss curves
    ax2 = plt.subplot(3, 3, 2)
    plt.plot(epochs, history.history['loss'], 'g-', label='Train Total Loss', linewidth=2)
    plt.plot(epochs, history.history['val_loss'], 'g--', label='Val Total Loss', linewidth=2)
    plt.plot(epochs, history.history['superclass_output_loss'], 'b-', alpha=0.6, label='Train Superclass Loss')
    plt.plot(epochs, history.history['fine_class_output_loss'], 'r-', alpha=0.6, label='Train Fine Class Loss')
    plt.title('Loss Curves: Hierarchical Model', fontweight='bold', fontsize=14)
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 3. Final accuracies bar chart
    ax3 = plt.subplot(3, 3, 3)
    accuracies = [
        eval_results['superclass_acc'],
        eval_results['fine_class_acc'],
        eval_results['fine_class_top5_acc'],
        eval_results['hierarchical_acc']
    ]
    labels = ['Superclass\n(Top-1)', 'Fine Class\n(Top-1)', 'Fine Class\n(Top-5)', 'Hierarchical\n(Both Correct)']
    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']

    bars = plt.bar(labels, accuracies, color=colors, alpha=0.8)
    plt.title('Final Test Accuracies', fontweight='bold', fontsize=14)
    plt.ylabel('Accuracy')
    plt.ylim(0, 1)

    # Add accuracy values on bars
    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

    # 4. Superclass confusion matrix
    ax4 = plt.subplot(3, 3, 4)
    cm_super = confusion_matrix(eval_results['superclass_true'], eval_results['superclass_preds'])
    cm_super_norm = cm_super.astype('float') / cm_super.sum(axis=1)[:, np.newaxis]
    sns.heatmap(cm_super_norm, annot=False, cmap='Blues', square=True)
    plt.title('Superclass Confusion Matrix', fontweight='bold', fontsize=12)
    plt.xlabel('Predicted Superclass')
    plt.ylabel('True Superclass')

    # 5. Top correctly predicted superclasses
    ax5 = plt.subplot(3, 3, 5)
    super_correct = np.diag(cm_super)
    top_super_indices = np.argsort(super_correct)[-10:]
    top_super_names = [cifar100_superclass_names[i] for i in top_super_indices]
    top_super_counts = super_correct[top_super_indices]

    plt.barh(range(len(top_super_names)), top_super_counts, color='lightblue', alpha=0.8)
    plt.yticks(range(len(top_super_names)), top_super_names)
    plt.xlabel('Correct Predictions')
    plt.title('Top 10 Superclass Performance', fontweight='bold', fontsize=12)
    plt.grid(True, alpha=0.3)

    # 6. Fine class performance by superclass
    ax6 = plt.subplot(3, 3, 6)
    superclass_fine_acc = []
    for super_idx in range(20):
        # Find fine classes belonging to this superclass
        fine_classes_in_super = [i for i, s in enumerate(cifar100_fine_to_superclass) if s == super_idx]
        if len(fine_classes_in_super) > 0:
            # Calculate accuracy for fine classes in this superclass
            mask = np.isin(eval_results['fine_class_true'], fine_classes_in_super)
            if np.sum(mask) > 0:
                acc = np.mean(np.array(eval_results['fine_class_preds'])[mask] ==
                             np.array(eval_results['fine_class_true'])[mask])
                superclass_fine_acc.append(acc)
            else:
                superclass_fine_acc.append(0)
        else:
            superclass_fine_acc.append(0)

    plt.bar(range(20), superclass_fine_acc, color='lightcoral', alpha=0.8)
    plt.xlabel('Superclass Index')
    plt.ylabel('Fine Class Accuracy')
    plt.title('Fine Class Accuracy by Superclass', fontweight='bold', fontsize=12)
    plt.xticks(range(0, 20, 2))
    plt.grid(True, alpha=0.3)

    # 7. Confidence intervals visualization
    ax7 = plt.subplot(3, 3, 7)
    metrics = ['Superclass', 'Fine Class', 'Fine Top-5', 'Hierarchical']
    ci_lower = []
    ci_upper = []

    for acc in accuracies:
        lower, upper = calculate_confidence_interval(acc, n_samples)
        ci_lower.append(lower)
        ci_upper.append(upper)

    x_pos = range(len(metrics))
    plt.errorbar(x_pos, accuracies,
                yerr=[np.array(accuracies) - np.array(ci_lower),
                      np.array(ci_upper) - np.array(accuracies)],
                fmt='o', capsize=5, capthick=2, markersize=8, linewidth=2)

    plt.xticks(x_pos, metrics, rotation=45)
    plt.ylabel('Accuracy')
    plt.title('95% Confidence Intervals', fontweight='bold', fontsize=12)
    plt.grid(True, alpha=0.3)

    # 8. Comparison with best single model
    ax8 = plt.subplot(3, 3, 8)
    best_single_top5 = 0.8  # Placeholder, should be passed as parameter

    comparison_data = {
        'Single Model (Top-1)': best_single_acc,
        'Single Model (Top-5)': best_single_top5,
        'Hierarchical (Top-1)': eval_results['fine_class_acc'],
        'Hierarchical (Top-5)': eval_results['fine_class_top5_acc']
    }

    bars = plt.bar(comparison_data.keys(), comparison_data.values(),
                   color=['lightblue', 'lightcyan', 'lightcoral', 'lightpink'], alpha=0.8)
    plt.title('Single vs Hierarchical Model', fontweight='bold', fontsize=12)
    plt.ylabel('Accuracy')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

    # Add values on bars
    for bar, acc in zip(bars, comparison_data.values()):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)

    # 9. Model summary text
    ax9 = plt.subplot(3, 3, 9)
    ax9.axis('off')

    # Calculate overfitting metrics
    final_train_super = history.history['superclass_output_accuracy'][-1]
    final_val_super = history.history['val_superclass_output_accuracy'][-1]
    final_train_fine = history.history['fine_class_output_accuracy'][-1]
    final_val_fine = history.history['val_fine_class_output_accuracy'][-1]

    overfitting_super = final_train_super - final_val_super
    overfitting_fine = final_train_fine - final_val_fine
    improvement = eval_results['fine_class_acc'] - best_single_acc

    summary_text = f"""HIERARCHICAL MODEL SUMMARY

Architecture: 2 (Deeper CNN)
Optimizer: Adam (0.001)
Regularization: L2 (0.001)
Dropout Rate: 0.3

FINAL RESULTS:
Superclass Acc: {eval_results['superclass_acc']:.4f}
Fine Class Acc: {eval_results['fine_class_acc']:.4f}
Fine Top-5 Acc: {eval_results['fine_class_top5_acc']:.4f}
Hierarchical Acc: {eval_results['hierarchical_acc']:.4f}

TRAINING:
Epochs: {len(epochs)}
Super Overfitting: {overfitting_super:.4f}
Fine Overfitting: {overfitting_fine:.4f}

Test Samples: {n_samples}

COMPARISON:
Best Single Model: {best_single_acc:.4f}
Hierarchical Model: {eval_results['fine_class_acc']:.4f}
Improvement: {improvement:+.4f}
    """

    plt.text(0.05, 0.95, summary_text, transform=ax9.transAxes, fontsize=9,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))

    plt.tight_layout()
    plt.show()

def save_results(all_results, filename='simplified_all_results.pkl'):
    """Save experiment results to pickle file."""
    with open(filename, 'wb') as f:
        pickle.dump(all_results, f)
    print(f"Results saved to '{filename}'")

def load_results(filename='simplified_all_results.pkl'):
    """Load experiment results from pickle file."""
    with open(filename, 'rb') as f:
        all_results = pickle.load(f)
    print(f"Loaded {len(all_results)} experiments from '{filename}'")
    return all_results