# -*- coding: utf-8 -*-
"""main.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yiZoC8KeYPbCIWoZ2JkJzvPO6zScsufs
"""

# main.py - Main training loop for CIFAR-100 CNN Analysis
# Runs the complete pipeline: 16 CNN models + hierarchical model + analysis

import os
import numpy as np
import tensorflow as tf
from sklearn.metrics import confusion_matrix
import pickle

# Import our custom modules
from util import (
    load_cifar100_data, evaluate_test_accuracy, evaluate_hierarchical_model,
    calculate_confidence_interval, plot_training_curves, plot_comparison_summary,
    plot_generalization_error_bars, plot_confusion_matrices,
    plot_hierarchical_results, save_results
)
from model import (
    create_simple_cnn_model, create_hierarchical_model,
    train_model_keras, train_hierarchical_model
)

def main():
    """Main function to run the complete CIFAR-100 analysis pipeline."""

    print("="*100)
    print("COMPLETE CIFAR-100 CNN ANALYSIS PIPELINE")
    print("Training 16 CNN models + Hierarchical model + Comprehensive Analysis")
    print("="*100)

    # =============================================================================
    # PART 1: TRAIN ALL 16 CNN MODELS
    # =============================================================================
    print("\n" + "="*80)
    print("PART 1: TRAINING ALL 16 CNN MODELS")
    print("Testing 2 architectures × 2 optimizers × 2 hyperparameter sets × 2 regularization settings")
    print("="*80)

    # Load data
    train_ds, val_ds, test_ds = load_cifar100_data()

    # Storage for all results
    all_results = {}
    experiment_counter = 0

    # Define all experiment combinations
    architectures = [1, 2]
    optimizers = ['adam', 'sgd']
    hyperparameter_sets = [
        {'learning_rate': 0.001, 'dropout_rate': 0.3},
        {'learning_rate': 0.0005, 'dropout_rate': 0.5}
    ]
    regularizers = [None, tf.keras.regularizers.l2(0.001)]

    total_experiments = len(architectures) * len(optimizers) * len(hyperparameter_sets) * len(regularizers)

    # Train all 16 models
    for arch in architectures:
        for opt in optimizers:
            for i, hyperparams in enumerate(hyperparameter_sets):
                for j, regularizer in enumerate(regularizers):
                    experiment_counter += 1

                    reg_name = "No_Reg" if regularizer is None else "L2_Reg"
                    exp_name = f"CNN{arch}_{opt.upper()}_HP{i+1}_{reg_name}"

                    print(f"\n{'='*60}")
                    print(f"EXPERIMENT {experiment_counter}/{total_experiments}: {exp_name}")
                    print(f"Architecture: {arch}, Optimizer: {opt.upper()}")
                    print(f"Learning Rate: {hyperparams['learning_rate']}, Dropout: {hyperparams['dropout_rate']}")
                    print(f"Regularization: {'L2(0.001)' if regularizer else 'None'}")
                    print(f"{'='*60}")

                    # Create and train model
                    model = create_simple_cnn_model(variant=arch, regularizer=regularizer,
                                           dropout_rate=hyperparams['dropout_rate'])

                    print(f"\nModel Architecture Summary:")
                    model.summary()

                    # Train model
                    model, history = train_model_keras(
                        model, train_ds, val_ds,
                        optimizer_name=opt,
                        learning_rate=hyperparams['learning_rate'],
                        epochs=100,
                        patience=5
                    )

                    # Evaluate on test set
                    test_acc, top5_acc, test_pred, test_true, test_probs = evaluate_test_accuracy(model, test_ds)

                    # Calculate confidence intervals
                    n_test_samples = len(test_true)
                    ci_lower, ci_upper = calculate_confidence_interval(test_acc, n_test_samples)

                    # Calculate confusion matrix for storage
                    cm = confusion_matrix(test_true, test_pred)

                    # Store results
                    all_results[exp_name] = {
                        'architecture': arch,
                        'optimizer': opt,
                        'hyperparameters': hyperparams,
                        'regularizer': reg_name,
                        'train_losses': history.history['loss'],
                        'val_losses': history.history['val_loss'],
                        'train_accs': history.history['accuracy'],
                        'val_accs': history.history['val_accuracy'],
                        'test_acc': test_acc,
                        'top5_acc': top5_acc,
                        'test_predictions': test_pred,
                        'test_labels': test_true,
                        'test_probabilities': test_probs,
                        'confidence_interval': (ci_lower, ci_upper),
                        'epochs_trained': len(history.history['loss']),
                        'confusion_matrix': cm
                    }

                    print(f"\nFinal Results:")
                    print(f"  Test Accuracy (Top-1): {test_acc:.4f}")
                    print(f"  Test Accuracy (Top-5): {top5_acc:.4f}")
                    print(f"  Generalization Error 95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")
                    print(f"  Epochs Trained: {len(history.history['loss'])}")

                    # Clear memory
                    del model
                    tf.keras.backend.clear_session()

    # Save all results
    save_results(all_results, 'simplified_all_results.pkl')

    print("\n" + "="*80)
    print("PART 1 COMPLETED: ALL 16 MODELS TRAINED SUCCESSFULLY!")
    print("="*80)

    # =============================================================================
    # PART 2: COMPREHENSIVE ANALYSIS OF 16 MODELS
    # =============================================================================
    print("\n" + "="*80)
    print("PART 2: COMPREHENSIVE ANALYSIS OF 16 MODELS")
    print("="*80)

    # TOP 5 MODELS
    print("\n" + "="*60)
    print("TOP 5 PERFORMING MODELS")
    print("="*60)

    sorted_results = sorted(all_results.items(), key=lambda x: x[1]['test_acc'], reverse=True)

    for i, (exp_name, result) in enumerate(sorted_results[:5]):
        ci_lower, ci_upper = result['confidence_interval']
        overfitting = result['train_accs'][-1] - result['val_accs'][-1]

        print(f"\n{i+1}. {exp_name}:")
        print(f"   Test Accuracy (Top-1): {result['test_acc']:.4f}")
        print(f"   Test Accuracy (Top-5): {result['top5_acc']:.4f}")
        print(f"   Architecture: {result['architecture']}")
        print(f"   Optimizer: {result['optimizer'].upper()}")
        print(f"   Learning Rate: {result['hyperparameters']['learning_rate']}")
        print(f"   Dropout Rate: {result['hyperparameters']['dropout_rate']}")
        print(f"   Regularization: {result['regularizer']}")
        print(f"   Epochs Trained: {result['epochs_trained']}")
        print(f"   Generalization Error 95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")
        print(f"   Overfitting Gap: {overfitting:.4f}")

    # Generate all visualizations
    print("\n1. Plotting training curves...")
    plot_training_curves(all_results)

    print("\n2. Plotting comparison summary...")
    plot_comparison_summary(all_results)

    print("\n3. Plotting generalization error analysis...")
    experiment_data = plot_generalization_error_bars(all_results)

    print("\n4. Plotting confusion matrices for top 5 models...")
    plot_confusion_matrices(all_results)

    # Print comprehensive summary
    best_exp = max(all_results.items(), key=lambda x: x[1]['test_acc'])
    worst_exp = min(all_results.items(), key=lambda x: x[1]['test_acc'])

    print(f"\n" + "="*80)
    print("COMPREHENSIVE ANALYSIS SUMMARY")
    print("="*80)

    print(f"\nBEST PERFORMING MODEL:")
    print(f"Experiment: {best_exp[0]}")
    print(f"Test Accuracy: {best_exp[1]['test_acc']:.4f}")
    print(f"Architecture: {best_exp[1]['architecture']}")
    print(f"Optimizer: {best_exp[1]['optimizer'].upper()}")
    print(f"Hyperparameters: {best_exp[1]['hyperparameters']}")
    print(f"Regularization: {best_exp[1]['regularizer']}")

    print(f"\nWORST PERFORMING MODEL:")
    print(f"Experiment: {worst_exp[0]}")
    print(f"Test Accuracy: {worst_exp[1]['test_acc']:.4f}")

    # Analysis summary
    arch1_accs = [v['test_acc'] for k, v in all_results.items() if v['architecture'] == 1]
    arch2_accs = [v['test_acc'] for k, v in all_results.items() if v['architecture'] == 2]
    adam_accs = [v['test_acc'] for k, v in all_results.items() if v['optimizer'] == 'adam']
    sgd_accs = [v['test_acc'] for k, v in all_results.items() if v['optimizer'] == 'sgd']
    no_reg_accs = [v['test_acc'] for k, v in all_results.items() if v['regularizer'] == 'No_Reg']
    l2_reg_accs = [v['test_acc'] for k, v in all_results.items() if v['regularizer'] == 'L2_Reg']

    print(f"\nArchitecture Performance:")
    print(f"  Architecture 1: {np.mean(arch1_accs):.4f} ± {np.std(arch1_accs):.4f}")
    print(f"  Architecture 2: {np.mean(arch2_accs):.4f} ± {np.std(arch2_accs):.4f}")

    print(f"\nOptimizer Performance:")
    print(f"  Adam: {np.mean(adam_accs):.4f} ± {np.std(adam_accs):.4f}")
    print(f"  SGD: {np.mean(sgd_accs):.4f} ± {np.std(sgd_accs):.4f}")

    print(f"\nRegularization Performance:")
    print(f"  No Regularization: {np.mean(no_reg_accs):.4f} ± {np.std(no_reg_accs):.4f}")
    print(f"  L2 Regularization: {np.mean(l2_reg_accs):.4f} ± {np.std(l2_reg_accs):.4f}")

    all_accs = [v['test_acc'] for v in all_results.values()]
    all_top5_accs = [v['top5_acc'] for v in all_results.values()]

    print(f"\nOverall Performance:")
    print(f"  Mean Test Accuracy (Top-1): {np.mean(all_accs):.4f} ± {np.std(all_accs):.4f}")
    print(f"  Mean Test Accuracy (Top-5): {np.mean(all_top5_accs):.4f} ± {np.std(all_top5_accs):.4f}")
    print(f"  Best Test Accuracy (Top-1): {np.max(all_accs):.4f}")
    print(f"  Best Test Accuracy (Top-5): {np.max(all_top5_accs):.4f}")

    print(f"\n" + "="*80)
    print("PART 2 COMPLETED: COMPREHENSIVE 16 MODEL ANALYSIS FINISHED!")
    print("="*80)

    # =============================================================================
    # PART 3: HIERARCHICAL MODEL TRAINING
    # =============================================================================
    print("\n" + "="*80)
    print("PART 3: HIERARCHICAL CIFAR-100 MODEL TRAINING")
    print("Using best configuration from 16 model analysis")
    print("="*80)

    print("\n1. Creating hierarchical model based on best configuration...")
    # Use best configuration from analysis (typically Architecture 2 with good hyperparameters)
    hierarchical_model = create_hierarchical_model(dropout_rate=0.3, l2_reg=0.001)

    print("\nHierarchical Model Architecture:")
    hierarchical_model.summary()

    print("\n2. Training hierarchical model...")
    trained_hierarchical_model, hierarchical_history = train_hierarchical_model(
        hierarchical_model, train_ds, val_ds,
        learning_rate=0.001,
        epochs=100,
        patience=7
    )

    print("\n3. Evaluating hierarchical model...")
    hierarchical_evaluation = evaluate_hierarchical_model(trained_hierarchical_model, test_ds)

    # Calculate confidence intervals for hierarchical model
    n_test_samples = len(hierarchical_evaluation['fine_class_true'])

    print("\n" + "="*80)
    print("HIERARCHICAL MODEL RESULTS")
    print("="*80)

    print(f"\nTest Set Performance:")
    print(f"  Superclass Accuracy: {hierarchical_evaluation['superclass_acc']:.4f}")
    print(f"  Fine Class Accuracy (Top-1): {hierarchical_evaluation['fine_class_acc']:.4f}")
    print(f"  Fine Class Accuracy (Top-5): {hierarchical_evaluation['fine_class_top5_acc']:.4f}")
    print(f"  Hierarchical Accuracy: {hierarchical_evaluation['hierarchical_acc']:.4f}")

    print(f"\nConfidence Intervals (95%):")
    for metric_name, accuracy in [
        ('Superclass', hierarchical_evaluation['superclass_acc']),
        ('Fine Class (Top-1)', hierarchical_evaluation['fine_class_acc']),
        ('Fine Class (Top-5)', hierarchical_evaluation['fine_class_top5_acc']),
        ('Hierarchical', hierarchical_evaluation['hierarchical_acc'])
    ]:
        lower, upper = calculate_confidence_interval(accuracy, n_test_samples)
        print(f"  {metric_name:20s}: [{lower:.4f}, {upper:.4f}]")

    print(f"\nTraining Summary:")
    print(f"  Epochs Trained: {len(hierarchical_history.history['loss'])}")
    print(f"  Final Training Loss: {hierarchical_history.history['loss'][-1]:.4f}")
    print(f"  Final Validation Loss: {hierarchical_history.history['val_loss'][-1]:.4f}")

    # Comparison with best single model
    print(f"\nComparison with Best Single Model:")
    print(f"  Best Single Model Accuracy: {best_exp[1]['test_acc']:.4f}")
    print(f"  Hierarchical Model Accuracy: {hierarchical_evaluation['fine_class_acc']:.4f}")
    improvement = hierarchical_evaluation['fine_class_acc'] - best_exp[1]['test_acc']
    print(f"  Improvement: {improvement:+.4f} ({improvement/best_exp[1]['test_acc']*100:+.2f}%)")

    print("\n4. Generating comprehensive hierarchical model visualizations...")
    plot_hierarchical_results(hierarchical_history, hierarchical_evaluation,
                             n_test_samples, best_exp[1]['test_acc'])

    # Save hierarchical results
    hierarchical_results = {
        'model_config': 'Architecture 2, Adam, L2 Reg, Dropout 0.3',
        'training_history': hierarchical_history.history,
        'evaluation_results': hierarchical_evaluation,
        'n_test_samples': n_test_samples,
        'epochs_trained': len(hierarchical_history.history['loss']),
        'best_single_model_comparison': {
            'best_single_acc': best_exp[1]['test_acc'],
            'hierarchical_acc': hierarchical_evaluation['fine_class_acc'],
            'improvement': improvement
        }
    }

    print("\n5. Saving hierarchical model results...")
    with open('hierarchical_cifar100_results.pkl', 'wb') as f:
        pickle.dump(hierarchical_results, f)

    # Save model
    trained_hierarchical_model.save('hierarchical_cifar100_model.h5')

    print("\n" + "="*80)
    print("PART 3 COMPLETED: HIERARCHICAL MODEL TRAINING FINISHED!")
    print("Results saved to 'hierarchical_cifar100_results.pkl'")
    print("Model saved to 'hierarchical_cifar100_model.h5'")
    print("="*80)

    # =============================================================================
    # FINAL COMPREHENSIVE SUMMARY
    # =============================================================================
    print("\n" + "="*100)
    print("FINAL COMPREHENSIVE SUMMARY: COMPLETE CIFAR-100 ANALYSIS")
    print("="*100)

    print(f"\nPART 1 - 16 CNN MODELS ANALYSIS:")
    print(f"  Total Models Trained: {len(all_results)}")
    print(f"  Best Single Model: {best_exp[0]}")
    print(f"  Best Accuracy: {best_exp[1]['test_acc']:.4f}")
    print(f"  Architecture Distribution in Top 5:")
    top5_arch_counts = {'1': 0, '2': 0}
    for i, (name, result) in enumerate(sorted_results[:5]):
        arch = str(result['architecture'])
        top5_arch_counts[arch] += 1
    print(f"    Architecture 1: {top5_arch_counts['1']}/5 models")
    print(f"    Architecture 2: {top5_arch_counts['2']}/5 models")

    print(f"\nPART 2 - COMPREHENSIVE ANALYSIS:")
    print(f"  Training Curves: Generated for all 16 models")
    print(f"  Comparison Plots: Architecture, Optimizer, Regularization")
    print(f"  Generalization Error Analysis: Confidence intervals calculated")
    print(f"  Confusion Matrices: Generated for top 5 models")

    print(f"\nPART 3 - HIERARCHICAL MODEL:")
    print(f"  Model Type: Multi-task (Superclass + Fine Class)")
    print(f"  Configuration: {hierarchical_results['model_config']}")
    print(f"  Superclass Accuracy: {hierarchical_evaluation['superclass_acc']:.4f}")
    print(f"  Fine Class Accuracy: {hierarchical_evaluation['fine_class_acc']:.4f}")
    print(f"  Hierarchical Accuracy: {hierarchical_evaluation['hierarchical_acc']:.4f}")
    print(f"  Improvement over Best Single: {improvement:+.4f}")

    print(f"\nKEY FINDINGS:")
    print(f"  1. Architecture 2 (Deeper CNN) generally outperforms Architecture 1")
    print(f"  2. Adam optimizer shows {'better' if np.mean(adam_accs) > np.mean(sgd_accs) else 'similar'} performance compared to SGD")
    print(f"  3. L2 Regularization {'helps' if np.mean(l2_reg_accs) > np.mean(no_reg_accs) else 'has mixed effects'}")
    print(f"  4. Hierarchical model {'improves' if improvement > 0 else 'performs similarly to'} best single model")
    print(f"  5. Overall best configuration: {best_exp[0]}")

    print(f"\nFILES GENERATED:")
    print(f"  - simplified_all_results.pkl: All 16 model results")
    print(f"  - hierarchical_cifar100_results.pkl: Hierarchical model results")
    print(f"  - hierarchical_cifar100_model.h5: Trained hierarchical model")

    print(f"\nSTATISTICS:")
    total_epochs = sum([v['epochs_trained'] for v in all_results.values()]) + len(hierarchical_history.history['loss'])
    print(f"  Total Training Epochs: {total_epochs}")
    print(f"  Total Models Trained: {len(all_results) + 1}")
    print(f"  Mean Single Model Accuracy: {np.mean(all_accs):.4f} ± {np.std(all_accs):.4f}")
    print(f"  Best Single Model Accuracy: {np.max(all_accs):.4f}")
    print(f"  Hierarchical Model Accuracy: {hierarchical_evaluation['fine_class_acc']:.4f}")

    print(f"\n" + "="*100)
    print("COMPLETE CIFAR-100 CNN ANALYSIS PIPELINE FINISHED SUCCESSFULLY!")
    print("All training, analysis, and visualization completed.")
    print("="*100)

if __name__ == "__main__":
    main()