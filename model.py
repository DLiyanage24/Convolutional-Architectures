# -*- coding: utf-8 -*-
"""model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17qT5R68YYBe5K_XFtH1Z9SSRuzh2tdgq
"""

# model.py - TensorFlow model definitions for CIFAR-100 CNN Analysis
# Contains CNN architectures and hierarchical model definitions

import tensorflow as tf
from util import preprocess_data, preprocess_hierarchical_data

def create_simple_cnn_model(variant=1, regularizer=None, dropout_rate=0.3):
    """
    Create simplified CNN model with two different architectures.

    Args:
        variant (int): Architecture variant (1 or 2)
        regularizer: L2 regularizer or None
        dropout_rate (float): Dropout rate

    Returns:
        tf.keras.Model: Compiled CNN model
    """
    model = tf.keras.Sequential()

    if variant == 1:  # Simple CNN Architecture
        # First conv+pooling block
        model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same',
                                        input_shape=(32, 32, 3), kernel_regularizer=regularizer))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Dropout(dropout_rate))

        # Second conv+pooling block
        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Dropout(dropout_rate))

        # Third conv+pooling block
        model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer))
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Dropout(dropout_rate))

        # Dense layers
        model.add(tf.keras.layers.Flatten())
        model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizer))
        model.add(tf.keras.layers.Dropout(dropout_rate))
        model.add(tf.keras.layers.Dense(100, activation='softmax'))

    elif variant == 2:  # Slightly Deeper CNN Architecture
        # First conv+pooling block
        model.add(tf.keras.layers.Conv2D(24, (3, 3), activation='relu', padding='same',
                                        input_shape=(32, 32, 3), kernel_regularizer=regularizer))
        model.add(tf.keras.layers.Conv2D(24, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Dropout(dropout_rate))

        # Second conv+pooling block
        model.add(tf.keras.layers.Conv2D(48, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer))
        model.add(tf.keras.layers.Conv2D(48, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Dropout(dropout_rate))

        # Third conv+pooling block
        model.add(tf.keras.layers.Conv2D(96, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer))
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Dropout(dropout_rate))

        # Dense layers
        model.add(tf.keras.layers.Flatten())
        model.add(tf.keras.layers.Dense(192, activation='relu', kernel_regularizer=regularizer))
        model.add(tf.keras.layers.Dropout(dropout_rate))
        model.add(tf.keras.layers.Dense(100, activation='softmax'))

    return model

def create_hierarchical_model(dropout_rate=0.3, l2_reg=0.001):
    """
    Create hierarchical model based on best configuration.

    Args:
        dropout_rate (float): Dropout rate
        l2_reg (float): L2 regularization strength

    Returns:
        tf.keras.Model: Hierarchical model with multiple outputs
    """
    # Shared feature extractor (Architecture 2 backbone)
    feature_input = tf.keras.layers.Input(shape=(32, 32, 3), name='image_input')

    # First conv block
    x = tf.keras.layers.Conv2D(24, (3, 3), activation='relu', padding='same',
                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(feature_input)
    x = tf.keras.layers.Conv2D(24, (3, 3), activation='relu', padding='same',
                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    x = tf.keras.layers.Dropout(dropout_rate)(x)

    # Second conv block
    x = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', padding='same',
                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)
    x = tf.keras.layers.Conv2D(48, (3, 3), activation='relu', padding='same',
                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    x = tf.keras.layers.Dropout(dropout_rate)(x)

    # Third conv block
    x = tf.keras.layers.Conv2D(96, (3, 3), activation='relu', padding='same',
                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    x = tf.keras.layers.Dropout(dropout_rate)(x)

    # Shared feature representation
    x = tf.keras.layers.Flatten()(x)
    shared_features = tf.keras.layers.Dense(192, activation='relu',
                                           kernel_regularizer=tf.keras.regularizers.l2(l2_reg),
                                           name='shared_features')(x)
    shared_features = tf.keras.layers.Dropout(dropout_rate)(shared_features)

    # Superclass prediction branch
    superclass_branch = tf.keras.layers.Dense(128, activation='relu',
                                             kernel_regularizer=tf.keras.regularizers.l2(l2_reg),
                                             name='superclass_dense')(shared_features)
    superclass_branch = tf.keras.layers.Dropout(dropout_rate)(superclass_branch)
    superclass_output = tf.keras.layers.Dense(20, activation='softmax',
                                             name='superclass_output')(superclass_branch)

    # Fine class prediction branch (conditioned on superclass)
    combined_features = tf.keras.layers.Concatenate(name='combined_features')([shared_features, superclass_output])

    fine_class_branch = tf.keras.layers.Dense(256, activation='relu',
                                             kernel_regularizer=tf.keras.regularizers.l2(l2_reg),
                                             name='fine_class_dense')(combined_features)
    fine_class_branch = tf.keras.layers.Dropout(dropout_rate)(fine_class_branch)
    fine_class_output = tf.keras.layers.Dense(100, activation='softmax',
                                             name='fine_class_output')(fine_class_branch)

    # Create model with multiple outputs
    model = tf.keras.Model(inputs=feature_input,
                          outputs=[superclass_output, fine_class_output],
                          name='hierarchical_cifar100_model')

    return model

def train_model_keras(model, train_ds, val_ds, optimizer_name='adam', learning_rate=0.001, epochs=100, patience=5):
    """
    Train model using Keras built-in training for efficiency.

    Args:
        model: Keras model to train
        train_ds: Training dataset
        val_ds: Validation dataset
        optimizer_name (str): Optimizer name ('adam' or 'sgd')
        learning_rate (float): Learning rate
        epochs (int): Maximum number of epochs
        patience (int): Early stopping patience

    Returns:
        tuple: (trained_model, history)
    """
    if optimizer_name.lower() == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer_name.lower() == 'sgd':
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
    else:
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    # Prepare datasets
    train_data = train_ds.map(preprocess_data).prefetch(tf.data.AUTOTUNE)
    val_data = val_ds.map(preprocess_data).prefetch(tf.data.AUTOTUNE)

    # Early stopping callback
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=patience,
        restore_best_weights=True,
        verbose=1
    )

    # Train model
    history = model.fit(
        train_data,
        validation_data=val_data,
        epochs=epochs,
        callbacks=[early_stopping],
        verbose=1
    )

    return model, history

def train_hierarchical_model(model, train_ds, val_ds, learning_rate=0.001, epochs=100, patience=7):
    """
    Train hierarchical model with multi-task learning.

    Args:
        model: Hierarchical Keras model
        train_ds: Training dataset
        val_ds: Validation dataset
        learning_rate (float): Learning rate
        epochs (int): Maximum number of epochs
        patience (int): Early stopping patience

    Returns:
        tuple: (trained_model, history)
    """

    # Compile model with multiple losses
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss={
            'superclass_output': 'categorical_crossentropy',
            'fine_class_output': 'categorical_crossentropy'
        },
        loss_weights={
            'superclass_output': 0.3,  # Lower weight for superclass (easier task)
            'fine_class_output': 0.7   # Higher weight for fine class (main task)
        },
        metrics={
            'superclass_output': ['accuracy'],
            'fine_class_output': ['accuracy']
        }
    )

    # Prepare datasets
    train_data = train_ds.map(preprocess_hierarchical_data).prefetch(tf.data.AUTOTUNE)
    val_data = val_ds.map(preprocess_hierarchical_data).prefetch(tf.data.AUTOTUNE)

    # Callbacks
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_fine_class_output_accuracy',
        patience=patience,
        restore_best_weights=True,
        verbose=1
    )

    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_fine_class_output_accuracy',
        factor=0.5,
        patience=3,
        verbose=1,
        min_lr=1e-7
    )

    # Train model
    print("Training hierarchical model...")
    history = model.fit(
        train_data,
        validation_data=val_data,
        epochs=epochs,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )

    return model, history